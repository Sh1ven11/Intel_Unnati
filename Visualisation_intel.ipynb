{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9741d4b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T15:09:28.809841Z",
     "iopub.status.busy": "2025-07-11T15:09:28.809526Z",
     "iopub.status.idle": "2025-07-11T15:09:56.681150Z",
     "shell.execute_reply": "2025-07-11T15:09:56.680157Z"
    },
    "papermill": {
     "duration": 27.876345,
     "end_time": "2025-07-11T15:09:56.682420",
     "exception": false,
     "start_time": "2025-07-11T15:09:28.806075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 15:09:30.528705: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752246570.740565      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752246570.801022      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "--- Preparing Visualization Dataset from TextOCR/test ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1752246585.846347      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 potential images for visualization.\n",
      "\n",
      "--- Loading Student Model ---\n",
      "Successfully loaded Student model from /kaggle/input/student/checkpoints/student_ckpt/ckpt-4\n",
      "\n",
      "--- Generating Visualizations using the Student Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1752246590.104338      19 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated visualization 1/10\n",
      "Generated visualization 2/10\n",
      "Generated visualization 3/10\n",
      "Generated visualization 4/10\n",
      "Generated visualization 5/10\n",
      "Generated visualization 6/10\n",
      "Generated visualization 7/10\n",
      "Generated visualization 8/10\n",
      "Generated visualization 9/10\n",
      "Generated visualization 10/10\n",
      "\n",
      "Successfully generated 10 visualizations in './visualizations'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import glob\n",
    "import logging\n",
    "\n",
    "# Suppress TensorFlow logging except for errors\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Set GPU memory growth if available\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# --- Configuration (Must match training script) ---\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = 1 # Use batch size 1 for visualization for simplicity\n",
    "BUFFER_SIZE = tf.data.AUTOTUNE\n",
    "\n",
    "# UPDATED: Path to your saved student model checkpoint\n",
    "# Based on your image, the 'student' output directory from Kaggle contains 'checkpoints'.\n",
    "# Assuming your script is run from the root of the Kaggle output.\n",
    "# If you are running this from /kaggle/working/, and the output is /kaggle/output/,\n",
    "# you might need to adjust BASE_OUTPUT_PATH.\n",
    "BASE_OUTPUT_PATH = '/kaggle/input/' # Adjust if your output is in a different location relative to where you run this script\n",
    "\n",
    "# The CHECKPOINT_DIR should point to the directory *containing* 'student_ckpt' and 'teacher_ckpt' folders.\n",
    "# In your case, this is '/kaggle/output/student/checkpoints'.\n",
    "CHECKPOINT_ROOT_DIR = os.path.join(BASE_OUTPUT_PATH, 'student', 'checkpoints')\n",
    "MODEL_TO_LOAD = 'student' # 'teacher' or 'student'\n",
    "\n",
    "# Dataset path for visualization\n",
    "# This should still point to your input dataset\n",
    "VISUALIZATION_DATASET_PATH = '/kaggle/input/a-curated-list-of-image-deblurring-datasets/DBlur/'\n",
    "VISUALIZATION_DATASET_NAME = 'TextOCR' # Or any other dataset you want to visualize from\n",
    "VISUALIZATION_DATASET_TYPE = 'test' # 'test' or 'validation'\n",
    "\n",
    "# Output directory for saved visualization images\n",
    "VISUALIZATION_OUTPUT_DIR = './visualizations' # This will create a 'visualizations' folder in your current working directory (e.g., /kaggle/working/visualizations)\n",
    "os.makedirs(VISUALIZATION_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Data Loading and Preprocessing (Copied from training script) ---\n",
    "\n",
    "def _load_single_image_py(image_path):\n",
    "    \"\"\"\n",
    "    Pure Python function to load and decode a single image.\n",
    "    Handles corrupt/malformed images by returning None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img_bytes = tf.io.read_file(image_path).numpy()\n",
    "        img = tf.image.decode_image(img_bytes, channels=CHANNELS, expand_animations=False)\n",
    "        if img is None or img.shape == (0, 0, 0): # Check for empty/malformed tensors\n",
    "            logging.warning(f\"Skipping malformed or empty image: {image_path.decode()}\")\n",
    "            return None\n",
    "        # Ensure image has 3 channels even if decoded with fewer (e.g., grayscale)\n",
    "        if img.shape[-1] != CHANNELS:\n",
    "            logging.warning(f\"Image has {img.shape[-1]} channels, expected {CHANNELS}: {image_path.decode()}. Converting to RGB.\")\n",
    "            if img.shape[-1] == 1:\n",
    "                img = tf.image.grayscale_to_rgb(img)\n",
    "            else:\n",
    "                return None # Too complex to auto-handle all cases, better to skip.\n",
    "        img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH], method=tf.image.ResizeMethod.BICUBIC)\n",
    "        img = tf.cast(img, tf.float32) / 255.0 # Normalize to [0, 1]\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error loading image {image_path.decode()}: {e}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "def _tf_py_function_wrapper(blur_path, sharp_path):\n",
    "    \"\"\"\n",
    "    Wrapper for tf.py_function to load image pairs and mark validity.\n",
    "    Returns (blurred_image, sharp_image, is_valid_pair).\n",
    "    \"\"\"\n",
    "    def load_and_validate(b_path, s_path):\n",
    "        blur_img = _load_single_image_py(b_path)\n",
    "        sharp_img = _load_single_image_py(s_path)\n",
    "        is_valid = blur_img is not None and sharp_img is not None\n",
    "        # Return dummy tensors if not valid to maintain shape, will be filtered later\n",
    "        return blur_img if is_valid else tf.zeros((IMG_HEIGHT, IMG_WIDTH, CHANNELS), dtype=tf.float32), \\\n",
    "               sharp_img if is_valid else tf.zeros((IMG_HEIGHT, IMG_WIDTH, CHANNELS), dtype=tf.float32), \\\n",
    "               tf.constant(is_valid, dtype=tf.bool)\n",
    "\n",
    "    blur_img, sharp_img, is_valid = tf.py_function(\n",
    "        load_and_validate,\n",
    "        [blur_path, sharp_path],\n",
    "        [tf.float32, tf.float32, tf.bool]\n",
    "    )\n",
    "\n",
    "    # Crucially, set the shape after tf.py_function\n",
    "    blur_img.set_shape([IMG_HEIGHT, IMG_WIDTH, CHANNELS])\n",
    "    sharp_img.set_shape([IMG_HEIGHT, IMG_WIDTH, CHANNELS])\n",
    "    is_valid.set_shape([]) # Scalar boolean\n",
    "\n",
    "    return blur_img, sharp_img, is_valid\n",
    "\n",
    "def create_image_dataset_for_viz(dataset_type, batch_size, dataset_name, base_path=VISUALIZATION_DATASET_PATH):\n",
    "    \"\"\"\n",
    "    Creates a tf.data.Dataset pipeline for visualization.\n",
    "    Only takes blur and sharp paths and doesn't apply augmentation.\n",
    "    \"\"\"\n",
    "    blur_dir = os.path.join(base_path, dataset_name, dataset_type, 'blur')\n",
    "    sharp_dir = os.path.join(base_path, dataset_name, dataset_type, 'sharp')\n",
    "\n",
    "    if not os.path.exists(blur_dir) or not os.path.exists(sharp_dir):\n",
    "        logging.error(f\"Visualization directories not found: {blur_dir} or {sharp_dir}\")\n",
    "        return None, 0\n",
    "\n",
    "    blur_files = sorted(glob.glob(os.path.join(blur_dir, '*.*')))\n",
    "    sharp_files = sorted(glob.glob(os.path.join(sharp_dir, '*.*')))\n",
    "\n",
    "    sharp_map = {os.path.basename(f): f for f in sharp_files}\n",
    "    matched_blur_paths = []\n",
    "    matched_sharp_paths = []\n",
    "\n",
    "    for blur_path in blur_files:\n",
    "        filename = os.path.basename(blur_path)\n",
    "        if filename in sharp_map:\n",
    "            matched_blur_paths.append(blur_path)\n",
    "            matched_sharp_paths.append(sharp_map[filename])\n",
    "        else:\n",
    "            logging.warning(f\"No matching sharp image found for {blur_path}. Skipping for visualization.\")\n",
    "\n",
    "    if not matched_blur_paths:\n",
    "        logging.error(f\"No valid image pairs found for visualization in {dataset_name}/{dataset_type}.\")\n",
    "        return None, 0\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((matched_blur_paths, matched_sharp_paths))\n",
    "    dataset = dataset.map(_tf_py_function_wrapper, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.filter(lambda blur_img, sharp_img, is_valid: is_valid) # Filter out invalid pairs\n",
    "    dataset = dataset.map(lambda blur_img, sharp_img, is_valid: (blur_img, sharp_img), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=BUFFER_SIZE)\n",
    "\n",
    "    return dataset, len(matched_blur_paths)\n",
    "\n",
    "# --- Model Architecture (Copied from training script) ---\n",
    "# It's crucial that the model architecture is identical to how it was saved.\n",
    "def conv_block(inputs, filters, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu', use_bn=True):\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size, strides=strides, padding=padding, use_bias=not use_bn)(inputs)\n",
    "    if use_bn:\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation(activation)(x)\n",
    "    return x\n",
    "\n",
    "def residual_block(inputs, filters, activation='relu'):\n",
    "    x = conv_block(inputs, filters, activation=activation)\n",
    "    x = conv_block(x, filters, activation=None) # No activation on last conv of residual block\n",
    "    x = tf.keras.layers.Add()([inputs, x])\n",
    "    x = tf.keras.layers.Activation(activation)(x)\n",
    "    return x\n",
    "\n",
    "def build_enhanced_unet(input_shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS), base_filters=64):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    conv1 = conv_block(inputs, base_filters)\n",
    "    conv1 = residual_block(conv1, base_filters)\n",
    "    pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1) # 128x128\n",
    "\n",
    "    conv2 = conv_block(pool1, base_filters * 2)\n",
    "    conv2 = residual_block(conv2, base_filters * 2)\n",
    "    pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2) # 64x64\n",
    "\n",
    "    conv3 = conv_block(pool2, base_filters * 4)\n",
    "    conv3 = residual_block(conv3, base_filters * 4)\n",
    "    pool3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3) # 32x32\n",
    "\n",
    "    conv4 = conv_block(pool3, base_filters * 8)\n",
    "    conv4 = residual_block(conv4, base_filters * 8)\n",
    "    pool4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv4) # 16x16\n",
    "\n",
    "    # Bottleneck\n",
    "    conv_bridge = conv_block(pool4, base_filters * 16)\n",
    "    conv_bridge = residual_block(conv_bridge, base_filters * 16)\n",
    "\n",
    "    # Decoder\n",
    "    up1 = tf.keras.layers.UpSampling2D(size=(2, 2))(conv_bridge) # 32x32\n",
    "    concat1 = tf.keras.layers.Concatenate()([up1, conv4])\n",
    "    conv5 = conv_block(concat1, base_filters * 8)\n",
    "    conv5 = residual_block(conv5, base_filters * 8)\n",
    "\n",
    "    up2 = tf.keras.layers.UpSampling2D(size=(2, 2))(conv5) # 64x64\n",
    "    concat2 = tf.keras.layers.Concatenate()([up2, conv3])\n",
    "    conv6 = conv_block(concat2, base_filters * 4)\n",
    "    conv6 = residual_block(conv6, base_filters * 4)\n",
    "\n",
    "    up3 = tf.keras.layers.UpSampling2D(size=(2, 2))(conv6) # 128x128\n",
    "    concat3 = tf.keras.layers.Concatenate()([up3, conv2])\n",
    "    conv7 = conv_block(concat3, base_filters * 2)\n",
    "    conv7 = residual_block(conv7, base_filters * 2)\n",
    "\n",
    "    up4 = tf.keras.layers.UpSampling2D(size=(2, 2))(conv7) # 256x256\n",
    "    concat4 = tf.keras.layers.Concatenate()([up4, conv1])\n",
    "    conv8 = conv_block(concat4, base_filters)\n",
    "    conv8 = residual_block(conv8, base_filters)\n",
    "\n",
    "    # Final output layer uses sigmoid for [0, 1] range\n",
    "    output = tf.keras.layers.Conv2D(CHANNELS, (1, 1), activation='sigmoid', padding='same')(conv8)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "    return model\n",
    "\n",
    "# --- Visualization Function ---\n",
    "def visualize_deblurring(model, dataset, output_dir, num_examples=5, filename_prefix=\"deblur_viz\"):\n",
    "    \"\"\"\n",
    "    Generates and saves visualizations of deblurring results.\n",
    "    \"\"\"\n",
    "    example_count = 0\n",
    "    \n",
    "    # Check if dataset is truly empty\n",
    "    try:\n",
    "        _ = next(iter(dataset.take(1)))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"Error: Visualization dataset is empty. Cannot generate images.\")\n",
    "        return\n",
    "\n",
    "    for blur_img_batch, sharp_img_batch in dataset:\n",
    "        if example_count >= num_examples:\n",
    "            break\n",
    "\n",
    "        deblurred_img_batch = model(blur_img_batch, training=False)\n",
    "\n",
    "        for i in range(blur_img_batch.shape[0]):\n",
    "            if example_count >= num_examples:\n",
    "                break\n",
    "\n",
    "            fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(blur_img_batch[i].numpy())\n",
    "            plt.title('Blurred Input')\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(deblurred_img_batch[i].numpy())\n",
    "            plt.title(f'{MODEL_TO_LOAD.capitalize()} Deblurred')\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow(sharp_img_batch[i].numpy())\n",
    "            plt.title('Ground Truth')\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.suptitle(f\"Deblurring Visualization - Example {example_count + 1}\", fontsize=16)\n",
    "            plt.savefig(os.path.join(output_dir, f'{filename_prefix}_example_{example_count:03d}.png'))\n",
    "            plt.close(fig)\n",
    "            example_count += 1\n",
    "            print(f\"Generated visualization {example_count}/{num_examples}\")\n",
    "\n",
    "    if example_count == 0:\n",
    "        print(\"No images were generated. Check dataset or model output.\")\n",
    "    else:\n",
    "        print(f\"\\nSuccessfully generated {example_count} visualizations in '{output_dir}'\")\n",
    "\n",
    "# --- Main Visualization Script ---\n",
    "def run_visualization():\n",
    "    print(f\"--- Preparing Visualization Dataset from {VISUALIZATION_DATASET_NAME}/{VISUALIZATION_DATASET_TYPE} ---\")\n",
    "    viz_dataset, num_viz_elements = create_image_dataset_for_viz(\n",
    "        VISUALIZATION_DATASET_TYPE, BATCH_SIZE, VISUALIZATION_DATASET_NAME\n",
    "    )\n",
    "\n",
    "    if viz_dataset is None or num_viz_elements == 0:\n",
    "        print(\"Exiting: No valid images found for visualization.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {num_viz_elements} potential images for visualization.\")\n",
    "\n",
    "    print(f\"\\n--- Loading {MODEL_TO_LOAD.capitalize()} Model ---\")\n",
    "    if MODEL_TO_LOAD == 'student':\n",
    "        model = build_enhanced_unet(base_filters=STUDENT_FILTERS)\n",
    "        # The checkpoint_prefix should point to the directory that *contains* the actual\n",
    "        # checkpoint files (e.g., ckpt-1.data-...). This is the 'student_ckpt' folder itself.\n",
    "        checkpoint_prefix = os.path.join(CHECKPOINT_ROOT_DIR, \"student_ckpt\")\n",
    "    elif MODEL_TO_LOAD == 'teacher':\n",
    "        model = build_enhanced_unet(base_filters=TEACHER_FILTERS)\n",
    "        checkpoint_prefix = os.path.join(CHECKPOINT_ROOT_DIR, \"teacher_ckpt\")\n",
    "    else:\n",
    "        print(\"Invalid model type specified. Choose 'teacher' or 'student'.\")\n",
    "        return\n",
    "\n",
    "    checkpoint = tf.train.Checkpoint(model=model)\n",
    "    manager = tf.train.CheckpointManager(checkpoint, checkpoint_prefix, max_to_keep=5) # Corrected usage\n",
    "\n",
    "    if manager.latest_checkpoint:\n",
    "        checkpoint.restore(manager.latest_checkpoint).expect_partial()\n",
    "        print(f\"Successfully loaded {MODEL_TO_LOAD.capitalize()} model from {manager.latest_checkpoint}\")\n",
    "    else:\n",
    "        print(f\"Error: No checkpoint found for {MODEL_TO_LOAD.capitalize()} at {checkpoint_prefix}. \"\n",
    "              \"Please ensure your training script saved checkpoints correctly and this path is accurate.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- Generating Visualizations using the {MODEL_TO_LOAD.capitalize()} Model ---\")\n",
    "    visualize_deblurring(model, viz_dataset, VISUALIZATION_OUTPUT_DIR, num_examples=10) # Generate 10 examples\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Define STUDENT_FILTERS and TEACHER_FILTERS here,\n",
    "    # as they are needed by build_enhanced_unet and might not be imported otherwise.\n",
    "    # These should match the values used during training.\n",
    "    TEACHER_FILTERS = 64\n",
    "    STUDENT_FILTERS = 48\n",
    "    run_visualization()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3055596,
     "sourceId": 5251537,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7847200,
     "sourceId": 12440059,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 35.249311,
   "end_time": "2025-07-11T15:09:59.809544",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-11T15:09:24.560233",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
